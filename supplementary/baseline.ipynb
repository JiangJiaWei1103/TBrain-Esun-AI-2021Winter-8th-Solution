{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "photographic-mechanics",
   "metadata": {},
   "source": [
    "# Baselines\n",
    "To validate the skill of models I create in the future, I need to implement a relatively reasonable or even strong baseline. Then, I can continually compare, tune and improve dataflow and pipeline cycles, including data processing, feature engineering and model architecture design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "driving-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from random import sample\n",
    "import matplotlib.pyplot as plt \n",
    "import math\n",
    "import json\n",
    "from time import process_time\n",
    "import pickle\n",
    "from itertools import product\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "module_path = os.path.abspath(\"..\")\n",
    "sys.path.append(module_path)\n",
    "from metadata import *\n",
    "from utils.evaluator import EvaluatorRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "impossible-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_RAW = \"../data/raw/raw_data.parquet\"\n",
    "DATA_PATH_TXN_AMTS = \"../data/raw/raw_txn_amts.parquet\"\n",
    "DATA_PATH_APC = \"../data/raw/raw_apc.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-confirmation",
   "metadata": {},
   "source": [
    "## Baseline 1 - Naive History Behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data \n",
    "df = pd.read_parquet(DATA_PATH_RAW, ['dt', 'chid', 'shop_tag', 'txn_cnt', 'txn_amt'])\n",
    "df = df[df['shop_tag'].isin(LEG_SHOP_TAG)]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "submission_template = pd.read_csv(\"../data/raw/chid_target.csv\")\n",
    "print(f\"#Customers to predict {len(submission_template)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum transaction amount based on each consumption category per customer\n",
    "txn_amt_sum = df.groupby(by=['chid', 'shop_tag']).agg({'txn_amt': 'sum'})\n",
    "display(txn_amt_sum.head())\n",
    "txn_amt_sum.reset_index(level='shop_tag', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use shop_tag with top3 frequency as the prediction of those\n",
    "# who haven't spent on legitimate shop tags\n",
    "leg_shop_tag_mode = txn_amt_sum['shop_tag'].value_counts().index[:3]\n",
    "submission = {k: [] for k in ['chid', 'top1', 'top2', 'top3']}\n",
    "# pd.DataFrame(columns=['chid', 'top1', 'top2', 'top3'])\n",
    "for chid in tqdm(submission_template['chid']):\n",
    "    chid_txn_amt_sum = txn_amt_sum[txn_amt_sum.index == chid]\n",
    "    if len(chid_txn_amt_sum) == 0:\n",
    "        shop_tag_top3 = leg_shop_tag_mode\n",
    "    else:\n",
    "        chid_txn_top3 = chid_txn_amt_sum.nlargest(3, columns='txn_amt')\n",
    "        shop_tag_top3 = chid_txn_top3['shop_tag'].values\n",
    "        if len(shop_tag_top3) > 3:\n",
    "            shop_tag_top3 = shop_tag_top3[:3]\n",
    "        elif len(shop_tag_top3) < 3:\n",
    "            shop_tag_top3 = np.pad(shop_tag_top3, \n",
    "                                   pad_width=(0, 3-len(shop_tag_top3)),\n",
    "                                   mode='edge')\n",
    "    submission['chid'].append(chid)\n",
    "    submission['top1'].append(shop_tag_top3[0])\n",
    "    submission['top2'].append(shop_tag_top3[1])\n",
    "    submission['top3'].append(shop_tag_top3[2])\n",
    "    \n",
    "submission = pd.DataFrame.from_dict(submission)\n",
    "submission.to_csv(\"./baseline1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-cooler",
   "metadata": {},
   "source": [
    "## Baseline 2 - Weighted Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data \n",
    "df = pd.read_parquet(DATA_PATH_RAW, ['dt', 'chid', 'shop_tag', 'txn_cnt', 'txn_amt'])\n",
    "submission_template = pd.read_csv(\"../data/raw/chid_target.csv\")\n",
    "print(f\"#Customers to predict {len(submission_template)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_consumes(gp):\n",
    "    '''Return number of samples in each group grouped by 'shop_tag'.\n",
    "    '''\n",
    "    return len(gp)\n",
    "\n",
    "def weighted_amt(df, txn_amt_mean2, n_consumes_thres):\n",
    "    '''Calculate weighted amount for each legitimate shop_tag.\n",
    "    \n",
    "    Parameters:\n",
    "        df: pd.DataFrame, information related to average transaction amount\n",
    "            and number of total consumption of each shop_tag\n",
    "        txn_amt_mean2: float, average over means of transaction amount of \n",
    "                       all the legitimate shop_tags\n",
    "        n_consumes_thres: float, threshold of number of consumption\n",
    "    \n",
    "    Return:\n",
    "        w_amt: pd.Series, weighted amount of each shop_tag\n",
    "    '''\n",
    "    v = df['n_consumes'] \n",
    "    R = df['txn_amt_mean']\n",
    "    w_amt = ((v/(v+n_consumes_thres) * R) + \n",
    "            (n_consumes_thres/(n_consumes_thres+v) * txn_amt_mean2))\n",
    "\n",
    "    return w_amt\n",
    " \n",
    "df_ = df.groupby(by=['shop_tag']).agg({'txn_amt': [np.mean], 'txn_cnt': ['count']})   # 'count' can be replaced\n",
    "                                                                                      # by n_consumes\n",
    "df_.columns = ['txn_amt_mean', 'n_consumes']\n",
    "txn_amt_mean2 = df_['txn_amt_mean'].mean()   # Take mean of mean of transaction amount\n",
    "n_consumes_thres = df_.quantile(0.8)['n_consumes']   # Threshold indicating minimum number of consumptions \n",
    "                                                     # needed for shop_tag to be taken in \n",
    "df_ = df_[df_['n_consumes'] >= n_consumes_thres]\n",
    "df_['weighted_amt'] = weighted_amt(df_, txn_amt_mean2, n_consumes_thres)\n",
    "df_.sort_values(by=['weighted_amt'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_tags_top3 = [shop_tag for shop_tag in df_.index if shop_tag in LEG_SHOP_TAG][:3]\n",
    "submission = np.tile(shop_tags_top3, reps=(len(submission_template), 1))\n",
    "submission = pd.DataFrame(submission, columns=[f'top{k}' for k in range(1, 4)])\n",
    "submission.insert(0, column='chid', value=submission_template['chid'])\n",
    "submission.to_csv(\"./baseline2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-norwegian",
   "metadata": {},
   "source": [
    "## Baseline 3 - TIFU-KNN Based \n",
    "### Concept\n",
    "1. Get predicting client representation using the concept of TIFU-KNN.\n",
    "2. Select the most potential `shop_tag`s that each client is willing to consume.\n",
    "    * Fixed threshold of #`shop_tag`s\n",
    "    * Client-specific threshold of #`shop_tag`s based on average #`shop_tag`s consumed per month \n",
    "    * Train separate classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/purch_maps.pkl\", 'rb') as f:\n",
    "     purch_maps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-coating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pif(purch_map, t1, t2):\n",
    "    '''Return personalized item frequency computed from the given time\n",
    "    interval.\n",
    "    \n",
    "    Parameters:\n",
    "        purch_map: ndarray, purchasing map indicating purchasing the \n",
    "                   shop_tag or not (i.e., represented by 0/1)\n",
    "        t1: int, time lower bound\n",
    "        t2: int, time upper bound\n",
    "    \n",
    "    Return:\n",
    "        pif: ndarray, personalized item frequency vector\n",
    "    '''\n",
    "    # Align with array index\n",
    "    purch_map = purch_map[t1:t2]\n",
    "    pif = np.sum(purch_map)\n",
    "    return pif\n",
    "\n",
    "class CliVecGenerator:\n",
    "    def __init__(self, purch_map_path, t1, t2, \n",
    "                 gp_size, decay_wt_g, decay_wt_b):\n",
    "        with open(purch_map_path, 'rb') as f:\n",
    "            self.purch_maps = pickle.load(f)\n",
    "        self.t1 = t1\n",
    "        self.t2 = t2  \n",
    "        self.gp_size = gp_size\n",
    "        self.decay_wt_g = decay_wt_g\n",
    "        self.decay_wt_b = decay_wt_b\n",
    "        self._setup()\n",
    "        \n",
    "    def get_client_vec(self, chid):\n",
    "        '''Return the client vector represented by fusing repeated purchase\n",
    "        pattern and collaborative one.\n",
    "\n",
    "        Parameters:\n",
    "            chid: int, client identifier\n",
    "\n",
    "        Return:\n",
    "            client_vec: ndarray, client vector representation\n",
    "        '''\n",
    "        purch_map = self.purch_maps[chid][self.t1:self.t2]\n",
    "        if self.first_gp_size != 0:\n",
    "            first_gp = purch_map[:self.first_gp_size]\n",
    "            first_gp = first_gp * self.wt_g[0]\n",
    "            first_gp = np.einsum('ij, i->j', first_gp, self.wt_b[self.first_gp_size:])\n",
    "            \n",
    "        normal_gps = np.reshape(purch_map[self.first_gp_size:], \n",
    "                                self.normal_gp_shape)   \n",
    "        normal_gps = np.einsum('ijk, i->jk', normal_gps, self.normal_gp_wt)\n",
    "        normal_gps = np.einsum('ij, i->j', normal_gps, self.wt_b)\n",
    "        client_vec = normal_gps if self.first_gp_size == 0 else first_gp + normal_gps\n",
    "    \n",
    "        return client_vec#np.expand_dims(client_vec, axis=0)\n",
    "    \n",
    "    def _setup(self):\n",
    "        self.n_baskets = self.t2 - self.t1   # See one month as one basket\n",
    "                                             # time interval is like [t1, t2)\n",
    "        self.n_gps = math.ceil(self.n_baskets / self.gp_size)\n",
    "        self.wt_g = [pow(self.decay_wt_g, p) for p in range(self.n_gps-1, -1, -1)]\n",
    "        self.wt_b = [pow(self.decay_wt_b, p) for p in range(self.gp_size-1, -1, -1)]\n",
    "        \n",
    "        self.first_gp_size = self.n_baskets % self.gp_size\n",
    "        if self.first_gp_size == 0:\n",
    "            # If each group has the same size\n",
    "            self.normal_gp_shape = (self.n_gps, self.gp_size, -1)\n",
    "            self.normal_gp_wt = self.wt_g\n",
    "        else:\n",
    "            self.normal_gp_shape = (self.n_gps-1, self.gp_size, -1)   # Ignore the first gp\n",
    "            self.normal_gp_wt = self.wt_g[1:]   # Ignore the first gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "textile-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv(params):\n",
    "    '''Do cross-validation and return the performance.\n",
    "    \n",
    "    Parameters:\n",
    "        params: dict, hyperparameters used in the current process\n",
    "    \n",
    "    Return:\n",
    "        NDCGs: list, NDCGs of different folds\n",
    "    '''\n",
    "    NDCGs = []\n",
    "    for t_interval in [(0, 23)]:#[(0, 20), (0, 21), (0, 22)]:\n",
    "        cli_vecs = get_cli_vecs(t1=t_interval[0], \n",
    "                                t2=t_interval[1], \n",
    "                                gp_size=params['gp_size'],\n",
    "                                decay_wt_g=params['decay_wt_g'], \n",
    "                                decay_wt_b=params['decay_wt_b'])\n",
    "        \n",
    "        pred_vecs = get_pred_vecs(cli_vecs=cli_vecs, \n",
    "                                  n_neighbor_candidates=params['n_neighbor_candidates'],\n",
    "                                  sim_measure=params['sim_measure'],\n",
    "                                  k=params['k'],\n",
    "                                  alpha=params['alpha'])\n",
    "        \n",
    "        t_range = (t_interval[0]+1, t_interval[1]+1)   # Algin with original 'dt' values\n",
    "        final_ranks_pred = get_final_ranks(pred_vecs, t_range=t_range)\n",
    "        evaluator = EvaluatorRank(data_path=\"./data/raw/raw_data.parquet\", \n",
    "                                  pred=final_ranks_pred, \n",
    "                                  t_next=t_interval[1]+1)\n",
    "        NDCGs.append(evaluator.evaluate())\n",
    "    \n",
    "    return final_ranks_pred, NDCGs\n",
    "\n",
    "def get_cli_vecs(t1, t2, gp_size, decay_wt_g, decay_wt_b):\n",
    "    cli_vec_generator = CliVecGenerator(\"./data/processed/purch_maps.pkl\", \n",
    "                                        t1=t1, \n",
    "                                        t2=t2, \n",
    "                                        gp_size=gp_size, \n",
    "                                        decay_wt_g=decay_wt_g, \n",
    "                                        decay_wt_b=decay_wt_b)\n",
    "    cli_vecs = {}\n",
    "    for chid in tqdm(cli_vec_generator.purch_maps.keys()):\n",
    "        cli_vecs[chid] = cli_vec_generator.get_client_vec(chid)\n",
    "    \n",
    "    return cli_vecs\n",
    "\n",
    "def get_pred_vecs(cli_vecs, n_neighbor_candidates, sim_measure, k, alpha):\n",
    "    pred = {}\n",
    "    cli_map = np.array([v for v in cli_vecs.values()])\n",
    "    \n",
    "    for chid, target_vec in tqdm(cli_vecs.items()):\n",
    "        sim_map = {}\n",
    "        un = np.zeros(N_SHOP_TAGS)\n",
    "        neighbor_candidates = sample(range(N_CLIENTS), n_neighbor_candidates)\n",
    "        \n",
    "#         neighbor_candidates = sample(cli_vecs.keys(), n_neighbor_candidates)\n",
    "        \n",
    "#         print(neighbor_candidates)\n",
    "#         t1 = process_time()\n",
    "        neighbor_mat = cli_map[neighbor_candidates]\n",
    "#         neighbor_mat = [cli_vecs[chid_] for chid_ in neighbor_candidates \n",
    "#                         if chid != chid_]\n",
    "#         neighbor_mat = np.array(neighbor_mat)\n",
    "#         print(neighbor_mat.shape)\n",
    "        \n",
    "#         t2 = process_time()\n",
    "#         print(f\"Neighbor matrix {t2-t1}sec\")\n",
    "\n",
    "#         t1 = process_time()\n",
    "        \n",
    "        if sim_measure == 'cos':\n",
    "            dot_sim = np.matmul(neighbor_mat, target_vec)\n",
    "            target_norm = np.linalg.norm(target_vec)\n",
    "            neighbor_norm = np.linalg.norm(neighbor_mat, axis=1)\n",
    "            sim_vec = dot_sim / (target_norm * neighbor_norm) \n",
    "        elif sim_measure == 'ed':\n",
    "            vec_sub = neighbor_mat - target_vec\n",
    "            sim_vec = np.linalg.norm(vec_sub, axis=1)\n",
    "#         t2 = process_time()\n",
    "#         print(f\"Sim measure {t2-t1}sec\")\n",
    "        \n",
    "#         t1 = process_time()\n",
    "        \n",
    "        sim_map = {chid_: sim for chid_, sim in zip(neighbor_candidates, sim_vec)}\n",
    "        sim_map = dict(sorted(sim_map.items(), \n",
    "                              key=lambda item: item[1], \n",
    "                              reverse=True))\n",
    "        neighbors = list(sim_map.keys())[:k]\n",
    "        \n",
    "#         t2 = process_time()\n",
    "#         print(f\"Take topk {t2-t1}sec\")\n",
    "        \n",
    "#         t1 = process_time()\n",
    "        \n",
    "        for n in neighbors:\n",
    "            un += cli_vecs[n+int(1e7)]\n",
    "        un = un / k\n",
    "        pred[chid] = alpha*target_vec + (1-alpha)*un\n",
    "        \n",
    "#         t2 = process_time()\n",
    "#         print(f\"Final pred {t2-t1}sec\")\n",
    "        del sim_map, un, neighbor_candidates, neighbor_mat, neighbors\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def get_final_ranks(pred, t_range):\n",
    "    df = pd.read_parquet(\"./data/raw/raw_data.parquet\", \n",
    "                         columns=['dt', 'chid', 'shop_tag', 'txn_amt'])\n",
    "    avg_shop_tags = get_avg_shop_tags_per_month(df[['dt', 'chid', 'shop_tag']], t_range)\n",
    "    avg_txn_amt = get_avg_txn_amt_per_basket(df, t_range)\n",
    "    \n",
    "    final_ranks = {col: [] for col in ['chid', 'top1', 'top2', 'top3']}\n",
    "    for chid, pred_vec in tqdm(pred.items()):\n",
    "        shop_tag_top3 = {}\n",
    "        k = round(avg_shop_tags[chid], 0)\n",
    "        txn_amt = avg_txn_amt[chid]\n",
    "        shop_tags_ranked = np.argsort(pred_vec)[::-1]   # Notice that this is idx list\n",
    "        try:\n",
    "            shop_tags_topk = shop_tags_ranked[:int(k)]\n",
    "        except:\n",
    "            print(k)\n",
    "            break\n",
    "        txn_amt_topk = {shop_tag: txn_amt[shop_tag] for shop_tag in shop_tags_topk}\n",
    "        shop_tags_topk_ranked = dict(\n",
    "                                    sorted(txn_amt_topk.items(), \n",
    "                                           key=lambda x: x[1], \n",
    "                                           reverse=True)\n",
    "                                ).keys()\n",
    "        for shop_tag in shop_tags_topk_ranked:\n",
    "            if shop_tag+1 in LEG_SHOP_TAGS:\n",
    "                shop_tag_top3[f'top{len(shop_tag_top3)+1}'] = shop_tag+1\n",
    "            if len(shop_tag_top3) == 3:\n",
    "                # If top3 shop tags have been captured so far\n",
    "                break\n",
    "        if len(shop_tag_top3) < 3:\n",
    "            for shop_tag in shop_tags_ranked[int(k):]:\n",
    "                if shop_tag+1 in LEG_SHOP_TAGS:\n",
    "                    shop_tag_top3[f'top{len(shop_tag_top3)+1}'] = shop_tag+1\n",
    "                if len(shop_tag_top3) == 3:\n",
    "                    # If top3 shop tags have been captured so far\n",
    "                    break\n",
    "        shop_tag_top3['chid'] = chid\n",
    "        for k, v in shop_tag_top3.items():\n",
    "            final_ranks[k].append(v)\n",
    "\n",
    "        del shop_tag_top3, txn_amt, shop_tags_ranked, \\\n",
    "            shop_tags_topk, txn_amt_topk, shop_tags_topk_ranked\n",
    "    \n",
    "    final_ranks = pd.DataFrame(final_ranks)\n",
    "    \n",
    "    return final_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_sets = {\n",
    "    'gp_size': [3, 6],\n",
    "    'decay_wt_g': [0.8, 0.5],\n",
    "    'decay_wt_b': [0.9, 0.6],\n",
    "    'alpha': [0.9, 0.6, 0.3],\n",
    "    'sim_measure': ['cos', 'ed'],\n",
    "    'k': [100, 500],\n",
    "    'n_neighbor_candidates': [250, 1000],\n",
    "}\n",
    "\n",
    "perf = []\n",
    "grid = list(product(*param_sets.values()))\n",
    "for params in grid:\n",
    "    params = {k: v for k, v in zip(param_sets.keys(), params)}\n",
    "    if params['k'] > params['n_neighbor_candidates']:\n",
    "        continue\n",
    "    elif params in done:\n",
    "        continue\n",
    "    ndcg = cv(params)\n",
    "    print(f\"{params}: {ndcg}\")\n",
    "    perf.append((params, ndcg))\n",
    "\n",
    "with open(\"./perf2.pkl\", 'wb') as f:\n",
    "    pickle.dump(perf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ranks, ndcg = cv({'gp_size': 3, 'decay_wt_g': 0.5, 'decay_wt_b': 0.9, 'alpha': 0.9, 'sim_measure': 'cos', 'k': 100, 'n_neighbor_candidates': 1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_version = 3\n",
    "cfg = {\n",
    "    't1': 0,\n",
    "    't2': 23,\n",
    "    'gp_size': 3,   # Take 3 to represent seasonal effect under the premise \n",
    "                    # that time point represents 'month'\n",
    "    # Changes across groups (seasons) may be dramatic, \n",
    "    # so take faster decaying (smaller wt)\n",
    "    'decay_wt_g': 0.7,\n",
    "    'decay_wt_b': 0.9,\n",
    "    'alpha': 0.7,\n",
    "    'sim_measure': 'cos',   # Similarity measurement\n",
    "    'k': 50,   # #Nearest neighbors\n",
    "    'n_neighbor_candidates': 250   # #Candidate neighbors to consider\n",
    "}\n",
    "# with open(f\"./exp/tifu-knn/version{v}/cfg.json\", 'w') as f:\n",
    "#     json.dump(cfg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-stand",
   "metadata": {},
   "outputs": [],
   "source": [
    "cli_vec_generator = CliVecGenerator(\"./data/processed/purch_maps.pkl\", \n",
    "                                    t1=cfg['t1'], \n",
    "                                    t2=cfg['t2'], \n",
    "                                    gp_size=cfg['gp_size'], \n",
    "                                    decay_wt_g=cfg['decay_wt_g'], \n",
    "                                    decay_wt_b=cfg['decay_wt_b'])\n",
    "cli_vecs = {}\n",
    "for chid in tqdm(cli_vec_generator.purch_maps.keys()):\n",
    "    cli_vecs[chid] = cli_vec_generator.get_client_vec(chid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = lambda v1, v2: np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "euclidean_sim = lambda v1, v2: np.linalg.norm(v1 - v2)\n",
    "\n",
    "k = cfg['k']\n",
    "alpha = cfg['alpha']\n",
    "\n",
    "pred = {}\n",
    "for chid, target_vec in tqdm(cli_vecs.items()):\n",
    "#     target_vec = np.squeeze(target_vec)\n",
    "    sim_map = {}\n",
    "    un = np.zeros(N_SHOP_TAGS)\n",
    "    neighbor_candidates = sample(cli_vecs.keys(), cfg['n_neighbor_candidates'])\n",
    "    \n",
    "#     neighbor_mat = [np.expand_dims(cli_vecs[chid_], axis=0) for chid_ in neighbor_candidates \n",
    "#                     if chid != chid_]\n",
    "    neighbor_mat = [cli_vecs[chid_] for chid_ in neighbor_candidates \n",
    "                    if chid != chid_]\n",
    "    neighbor_mat = np.array(neighbor_mat)\n",
    "#     neighbor_mat = np.array(neighbor_mat)\n",
    "#     neighbor_mat = np.vstack(neighbor_mat)\n",
    "#     neighbor_mat = np.concatenate(neighbor_mat, axis=0)\n",
    "    \n",
    "    if cfg['sim_measure'] == 'cos':\n",
    "        dot_sim = np.matmul(neighbor_mat, target_vec)\n",
    "        target_norm = np.linalg.norm(target_vec)\n",
    "        neighbor_norm = np.linalg.norm(neighbor_mat, axis=1)\n",
    "        sim_vec = dot_sim / (target_norm * neighbor_norm) \n",
    "        sim_map = {chid_: sim for chid_, sim in zip(neighbor_candidates, sim_vec)}\n",
    "    elif cfg['sim_measure'] == 'ed':\n",
    "        pass\n",
    "    \n",
    "#     for chid_  in neighbor_candidates:\n",
    "#         if chid == chid_:\n",
    "#             continue\n",
    "#         sim_map[chid_] = cosine_sim(target_vec, cli_vecs[chid_])\n",
    "     \n",
    "#     t2 = process_time()\n",
    "#     print(f\"Sim measure takes {t2-t1}s.\")\n",
    "    \n",
    "    sim_map = dict(sorted(sim_map.items(), \n",
    "                          key=lambda item: item[1], \n",
    "                          reverse=True))\n",
    "    neighbors = list(sim_map.keys())[:k]\n",
    "    for n in neighbors:\n",
    "        un += cli_vecs[n]#np.squeeze(cli_vecs[n])\n",
    "    un = un / k\n",
    "    pred[chid] = alpha*target_vec + (1-alpha)*un\n",
    "    del sim_map, un, neighbor_candidates, neighbor_mat, neighbors\n",
    "\n",
    "# with open(\"./tifu_knn_v2.pkl\", 'wb') as f:\n",
    "#     pickle.dump(pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-machine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "df = pd.read_parquet(DATA_PATH_RAW, columns=['dt', 'chid', 'shop_tag', 'txn_amt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the final ranks\n",
    "k = None   # Fixed number indicating that top-k potential items will be considered \n",
    "t_range = (1, 24)   # Set time interval to generate features\n",
    "                    # Before coming up with val scheme, set to full\n",
    "                    # And observe performance using leaderboard (no cv)\n",
    "                    # tmp solution (bad)\n",
    "\n",
    "avg_shop_tags = get_avg_shop_tags_per_month(df[['dt', 'chid', 'shop_tag']], t_range)\n",
    "avg_txn_amt = get_avg_txn_amt_per_basket(df, t_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "submission_template = pd.read_csv(\"./data/raw/chid_target.csv\")\n",
    "submission = {col: [] for col in submission_template.columns}\n",
    "for chid, pred_vec in tqdm(pred.items()):\n",
    "    shop_tag_top3 = {}\n",
    "    k = round(avg_shop_tags[chid], 0)\n",
    "    txn_amt = avg_txn_amt[chid]\n",
    "    shop_tags_ranked = np.argsort(pred_vec)[::-1]   # Notice that this is idx list\n",
    "    try:\n",
    "        shop_tags_topk = shop_tags_ranked[:int(k)]\n",
    "    except:\n",
    "        print(k)\n",
    "        break\n",
    "    txn_amt_topk = {shop_tag: txn_amt[shop_tag] for shop_tag in shop_tags_topk}\n",
    "    shop_tags_topk_ranked = dict(\n",
    "                                sorted(txn_amt_topk.items(), \n",
    "                                       key=lambda x: x[1], \n",
    "                                       reverse=True)\n",
    "                            ).keys()\n",
    "    for shop_tag in shop_tags_topk_ranked:\n",
    "        if shop_tag+1 in LEG_SHOP_TAGS:\n",
    "            shop_tag_top3[f'top{len(shop_tag_top3)+1}'] = shop_tag+1\n",
    "        if len(shop_tag_top3) == 3:\n",
    "            # If top3 shop tags have been captured so far\n",
    "            break\n",
    "    if len(shop_tag_top3) < 3:\n",
    "        for shop_tag in shop_tags_ranked[int(k):]:\n",
    "            if shop_tag+1 in LEG_SHOP_TAGS:\n",
    "                shop_tag_top3[f'top{len(shop_tag_top3)+1}'] = shop_tag+1\n",
    "            if len(shop_tag_top3) == 3:\n",
    "                # If top3 shop tags have been captured so far\n",
    "                break\n",
    "    shop_tag_top3['chid'] = chid\n",
    "    for k, v in shop_tag_top3.items():\n",
    "        submission[k].append(v)\n",
    "\n",
    "    del shop_tag_top3, txn_amt, shop_tags_ranked, \\\n",
    "        shop_tags_topk, txn_amt_topk, shop_tags_topk_ranked\n",
    "    \n",
    "pred = pd.DataFrame(submission)\n",
    "submission.to_csv(\"./baseline3.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sinopac",
   "language": "python",
   "name": "sinopac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
