# XGB model hyperparameters
params:
    # General
    booster: gbtree
    verbosity: 2
    # Tree booster
    tree_method: gpu_hist
    learning_rate: 0.01
    gamma: 0
    max_depth: 6
    min_child_weight: 1
    subsample: 0.7
    colsample_bytree: 0.7
    alpha: 2   # l1 reg
    lambda: 7   # l2 reg
    # Learning task
    objective: 'multi:softprob'
    num_class: 16
    eval_metric: mlogloss

train:
    num_iterations: 10000
    verbose_eval: 250
    es_rounds: 500   # Early stopping rounds
    